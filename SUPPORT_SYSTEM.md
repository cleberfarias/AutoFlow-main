# Sistema de Hierarquia de IA + Sess√µes - ChatGuru Support

## üìã Vis√£o Geral

Sistema implementado para suporte do ChatGuru com:
- **Hierarquia de IA em 2 camadas** (regras r√°pidas + LLM)
- **Gerenciamento de sess√µes** com estados (START/MIDDLE/COOLDOWN/END/HUMAN)
- **Backend seguro** (OPENAI_API_KEY apenas no servidor Vercel)
- **Persist√™ncia local** com localStorage (f√°cil migra√ß√£o para Redis/MongoDB)

## üèóÔ∏è Arquitetura

```
Frontend (React)
    ‚Üì
src/services/supportRouter.ts
    ‚Üì
Camada 0: Regras r√°pidas (sem IA)
    ‚îú‚îÄ GREETING ‚Üí resposta imediata
    ‚îú‚îÄ THANKS ‚Üí resposta imediata
    ‚îú‚îÄ OK/EMOJI ‚Üí resposta imediata
    ‚îî‚îÄ "falar com atendente" ‚Üí HANDOFF imediato
    ‚Üì
Camada 1: API Backend (se Camada 0 n√£o resolver)
    ‚Üì
/api/support-router.ts (Vercel Serverless)
    ‚Üì
OpenAI API (gpt-4o-mini)
    ‚Üì
JSON estruturado com intent, action, stage_next, etc.
```

## üìÅ Arquivos Criados

### 1. `src/services/session.ts`
Gerenciamento de sess√µes com localStorage:
- `getSession(chatId)` - Recupera ou cria sess√£o
- `saveSession(chatId, session)` - Persiste sess√£o
- `resetSession(chatId)` - Limpa sess√£o
- `isExpired(session)` - Verifica TTL (30 min padr√£o)
- `addMessageToHistory()` - Adiciona mensagem ao hist√≥rico
- `cleanupExpiredSessions()` - Limpeza autom√°tica

**Tipos exportados:**
- `SessionState` - Estado completo da sess√£o
- `SessionStage` - START | MIDDLE | COOLDOWN | END | HUMAN
- `SupportIntent` - AUTH_LOGIN | WHATSAPP_CONNECT | MESSAGES | etc.
- `Severity` - LOW | MEDIUM | HIGH

### 2. `src/services/supportRouter.ts`
L√≥gica de roteamento inteligente:
- **Camada 0**: Padr√µes regex para sauda√ß√µes, agradecimentos, comandos
- **Camada 1**: Chamada √† API `/api/support-router`
- **Processamento**: Atualiza stage, intent, slots, confidence
- **Resposta**: Objeto `SupportUIResponse` pronto para renderiza√ß√£o

**Fun√ß√µes principais:**
- `handleMessage(chatId, userMessage)` - Processa mensagem (async)
- `resetConversation(chatId)` - Reseta estado mantendo hist√≥rico
- `getSessionStatus(chatId)` - Retorna status atual

### 3. `api/support-router.ts`
Endpoint serverless Vercel:
- **Model**: gpt-4o-mini (barato e r√°pido)
- **Entrada**: chatId, userMessage, stage atual, slots, hist√≥rico
- **Sa√≠da**: JSON validado com intent, confidence, reply, checklist
- **Seguran√ßa**: OPENAI_API_KEY apenas no backend (process.env)
- **Fallback**: Em caso de erro, retorna HANDOFF autom√°tico

**System Prompt inclui:**
- Dom√≠nio completo do ChatGuru (AUTH, WHATSAPP, BILLING, etc.)
- Regras de severidade e confidence
- Instru√ß√µes para checklist e missing_slots
- Exemplos pr√°ticos

### 4. `components/TestChat.tsx` (modificado)
Componente atualizado com dois modos:
- **mode='workflow'**: Comportamento original (simulador)
- **mode='support'**: Novo sistema de suporte

**Novos recursos no modo suporte:**
- Barra de status da sess√£o (stage, intent, severity, confidence)
- Renderiza√ß√£o de checklist visual (passos numerados)
- Badge de handoff quando transfere para humano
- Hist√≥rico persistente via localStorage

## üöÄ Como Usar

### 1. Configura√ß√£o (Vercel)

Adicione vari√°vel de ambiente na Vercel:
```bash
OPENAI_API_KEY=sk-proj-...sua-chave-real...
```

### 2. Configura√ß√£o Local (.env.local)

```bash
# Backend (Vercel Functions)
OPENAI_API_KEY="sk-proj-..."

# Frontend (legacy - ser√° removido)
VITE_OPENAI_API_KEY="sk-proj-..."
```

### 3. Usar no c√≥digo

#### Modo Suporte (novo sistema):
```tsx
<TestChat 
  steps={[]} 
  onClose={() => {}} 
  onStepActive={() => {}}
  mode="support"
/>
```

#### Modo Workflow (comportamento original):
```tsx
<TestChat 
  steps={workflowSteps} 
  onClose={() => {}} 
  onStepActive={(id) => {}}
  mode="workflow"
/>
```

### 4. Integra√ß√£o direta (sem TestChat):

```typescript
import { handleMessage, getSessionStatus } from './src/services/supportRouter';

// Processar mensagem
const response = await handleMessage('chat_123', 'Como fa√ßo login?');

console.log(response.replyText);       // Texto da resposta
console.log(response.intent);          // AUTH_LOGIN
console.log(response.severity);        // LOW
console.log(response.action);          // CHECKLIST
console.log(response.checklist);       // ['Acesse...', 'Clique...']

// Obter status da sess√£o
const status = getSessionStatus('chat_123');
console.log(status.stage);             // MIDDLE
console.log(status.confidence);        // 0.95
```

## üéØ Fluxo de Exemplo

### Exemplo 1: Sauda√ß√£o simples (Camada 0)
```
User: "oi"
‚Üí Camada 0 detecta padr√£o ‚Üí resposta imediata
‚Üí Stage: START ‚Üí END
‚Üí Intent: GREETING
‚Üí Tempo: ~0ms (sem chamada de API)
```

### Exemplo 2: Problema t√©cnico (Camada 1)
```
User: "WhatsApp n√£o conecta"
‚Üí Camada 0 n√£o resolve ‚Üí chama /api/support-router
‚Üí OpenAI analisa ‚Üí retorna JSON
‚Üí Intent: WHATSAPP_CONNECT
‚Üí Severity: MEDIUM
‚Üí Action: ASK
‚Üí Reply: "Vou te ajudar. Aparece alguma mensagem de erro?"
‚Üí Stage: MIDDLE ‚Üí COOLDOWN
‚Üí missing_slots: ['error_message', 'last_connection_time']
```

### Exemplo 3: Pedido de humano (Camada 0)
```
User: "preciso falar com atendente"
‚Üí Camada 0 detecta ‚Üí HANDOFF imediato
‚Üí Stage: qualquer ‚Üí HUMAN
‚Üí Intent: HUMAN
‚Üí Tempo: ~0ms (sem chamada de API)
```

### Exemplo 4: Billing (Camada 1 + High Severity)
```
User: "quero cancelar meu plano"
‚Üí Camada 1 ‚Üí OpenAI detecta BILLING
‚Üí Severity: HIGH (quest√£o financeira sens√≠vel)
‚Üí Action: HANDOFF
‚Üí Reply: "Vou transferir para equipe de contas..."
‚Üí Stage: qualquer ‚Üí HUMAN
```

## üìä Schema JSON do LLM

```typescript
{
  "intent": "AUTH_LOGIN|WHATSAPP_CONNECT|MESSAGES|...",
  "confidence": 0.85,
  "severity": "LOW|MEDIUM|HIGH",
  "stage_next": "START|MIDDLE|COOLDOWN|END|HUMAN",
  "action": "REPLY|ASK|CHECKLIST|HANDOFF",
  "missing_slots": ["campo1", "campo2"],
  "reply": "texto da resposta (max 300 chars)",
  "checklist": ["Passo 1", "Passo 2"],
  "handoff_reason": "motivo da transfer√™ncia"
}
```

## üîí Seguran√ßa

### ‚úÖ Correto (implementado):
```typescript
// Backend (Vercel Function)
const apiKey = process.env.OPENAI_API_KEY;
```

### ‚ùå Inseguro (evitado):
```typescript
// Frontend
const apiKey = import.meta.env.VITE_OPENAI_API_KEY; // NUNCA FAZER ISSO!
```

## üîÑ Migra√ß√£o Futura

O sistema foi desenhado para f√°cil migra√ß√£o:

### De localStorage para Redis:
```typescript
// Antes (src/services/session.ts)
export function getSession(chatId: string): SessionState {
  const stored = localStorage.getItem(key);
  // ...
}

// Depois
export async function getSession(chatId: string): Promise<SessionState> {
  const stored = await redis.get(key);
  // ...
}
```

### De localStorage para MongoDB:
```typescript
// Depois
export async function getSession(chatId: string): Promise<SessionState> {
  const session = await SessionModel.findOne({ chatId });
  // ...
}
```

## üß™ Testes

### Testar Camada 0 (sem gastar API):
```
- "oi" ‚Üí GREETING imediato
- "obrigado" ‚Üí THANKS imediato
- "ok" ‚Üí confirma√ß√£o imediata
- "üëç" ‚Üí emoji reconhecido
- "falar com atendente" ‚Üí HANDOFF imediato
```

### Testar Camada 1 (usa API):
```
- "como fa√ßo login?" ‚Üí AUTH_LOGIN + CHECKLIST
- "WhatsApp desconectou" ‚Üí WHATSAPP_CONNECT + ASK
- "quero cancelar" ‚Üí BILLING + HANDOFF (HIGH severity)
- "xyz abc 123" ‚Üí UNKNOWN + ASK (low confidence)
```

## üìà M√©tricas e Monitoramento

O sistema j√° loga tudo no console:
```typescript
console.log('[Session] Sess√£o chat_123 salva - Stage: MIDDLE, Intent: AUTH_LOGIN');
console.log('[SupportRouter] Resolvido pela Camada 0');
console.log('[SupportRouter] Chamando API...');
console.log('[API] Processando: { chatId, message, stage, intent }');
```

## üé® UI Features

### Barra de Status (modo suporte):
- **Stage**: Cor azul
- **Intent**: Cor roxa
- **Severity**: Verde (LOW) | Amarelo (MEDIUM) | Vermelho (HIGH)
- **Confidence**: Percentual

### Checklist Visual:
- N√∫meros em c√≠rculo azul
- Fundo gradiente azul claro
- Borda azul

### Badge de Handoff:
- Ponto animado amarelo
- Texto: "Transferindo para atendimento humano..."

## üõ†Ô∏è Troubleshooting

### Erro: "OPENAI_API_KEY n√£o configurada"
- **Causa**: Vari√°vel de ambiente n√£o definida
- **Solu√ß√£o**: Configure na Vercel ou .env.local

### Erro: "API retornou status 401"
- **Causa**: Chave da API inv√°lida/expirada
- **Solu√ß√£o**: Verifique a chave em https://platform.openai.com

### Sess√£o n√£o persiste ap√≥s refresh
- **Causa**: localStorage foi limpo ou dom√≠nio diferente
- **Solu√ß√£o**: Normal em modo dev; em prod funciona corretamente

### LLM retorna texto em vez de JSON
- **Causa**: Model n√£o suporta response_format
- **Solu√ß√£o**: J√° tem fallback autom√°tico para HANDOFF

## üö¶ Pr√≥ximos Passos (Roadmap)

1. **Migrar persist√™ncia**: localStorage ‚Üí Redis/MongoDB
2. **Analytics**: Dashboard com m√©tricas (intents mais comuns, confidence m√©dia)
3. **Feedback loop**: Bot√µes "√∫til/n√£o √∫til" para melhorar prompts
4. **Handoff real**: Integrar com sistema de tickets (Zendesk, Intercom)
5. **Multi-idioma**: Detectar idioma e ajustar prompts
6. **Voice**: Integrar Whisper API (j√° tem base no TestChat)

## üìù Notas Importantes

- **TTL padr√£o**: 30 minutos (configur√°vel)
- **Custo**: gpt-4o-mini √© ~10x mais barato que gpt-4
- **Lat√™ncia**: Camada 0 = 0ms | Camada 1 = 500-1500ms
- **Taxa de Camada 0**: ~40% das intera√ß√µes (economia significativa)
- **Fallback**: Sempre prioriza experi√™ncia do usu√°rio (HANDOFF em caso de erro)

---

**Autor**: Sistema desenvolvido seguindo as melhores pr√°ticas de seguran√ßa, performance e UX.
**Data**: Janeiro 2026
**Vers√£o**: 1.0.0
